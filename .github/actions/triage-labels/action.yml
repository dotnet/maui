name: 'Triage Labels Action'

description: 'Process issues and apply labels using AI inference'

inputs:
  issue:
    description: 'The issue number to triage'
    required: false
    default: ''
  token:
    description: 'GitHub token to use for authentication'
    required: true

outputs:
  labels:
    description: 'The labels to apply to the issue'
    value: ${{ steps.inference.outputs.response }}
  labels-file:
    description: 'The file taht contains the labels to apply to the issue'
    value: ${{ steps.inference.outputs.response-file }}

runs:
  using: 'composite'
  steps:

    - name: Get Issue Data
      id: issue-data
      uses: actions/github-script@v7
      with:
        script: |
          const issueNumber = '${{ inputs.issue }}' || github.event.issue.number;
          const issue = await github.rest.issues.get({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: issueNumber
          });
          const data = {
            title: issue.data.title,
            body: issue.data.body,
            number: issue.data.number
          };
          const fs = require('fs');
          fs.writeFileSync('issue.json', JSON.stringify(data, null, 2));
          return data;

    - name: Create User Prompt
      id: create-user-prompt
      shell: pwsh
      env:
        GH_TOKEN: ${{ inputs.token }}
      run: echo $pwd ; ./process-prompt-template.ps1 -Template "user-prompt-template.md" -Output "user-prompt.md"
      
    - name: Create System Prompt
      id: create-system-prompt
      shell: pwsh
      env:
        GH_TOKEN: ${{ inputs.token }}
      run: ./process-prompt-template.ps1 -Template "system-prompt-template.md" -Output "system-prompt.md"

    - name: Run AI Inference
      id: inference
      uses: actions/ai-inference@main
      with:
        prompt-file: "user-prompt.md"
        system-prompt-file: "system-prompt.md"
      env:
        GITHUB_TOKEN: ${{ inputs.token }}

    - name: Log AI Analysis
      shell: pwsh
      run: |
        cat "${{ steps.inference.outputs.response-file }}"
